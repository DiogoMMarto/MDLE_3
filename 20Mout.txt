.\.venv\Scripts\spark-submit --driver-memory 24g --conf spark.executor.memoryOverhead=2048m --conf spark.driver.memoryOverhead=1024m --conf spark.sql.shuffle.partitions=200 .\3B.py .\ml-20m\ml-20m\ratings.csv --bucket_length 0.01 --num_hash_tables 30 --distance_threshold 0.9 --discard 10000
25/06/01 19:40:41 WARN Shell: Did not find winutils.exe: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems
25/06/01 19:40:41 INFO SparkContext: Running Spark version 3.5.5
25/06/01 19:40:41 INFO SparkContext: OS info Windows 11, 10.0, amd64
25/06/01 19:40:41 INFO SparkContext: Java version 11.0.27
25/06/01 19:40:42 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/06/01 19:40:42 INFO ResourceUtils: ==============================================================
25/06/01 19:40:42 INFO ResourceUtils: No custom resources configured for spark.driver.
25/06/01 19:40:42 INFO ResourceUtils: ==============================================================
25/06/01 19:40:42 INFO SparkContext: Submitted application: LSH MovieLens Recommender (file: ratings.csv)
25/06/01 19:40:42 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(memoryOverhead -> name: memoryOverhead, amount: 2048, script: , vendor: , cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
25/06/01 19:40:42 INFO ResourceProfile: Limiting resource is cpu
25/06/01 19:40:42 INFO ResourceProfileManager: Added ResourceProfile id: 0
25/06/01 19:40:42 INFO SecurityManager: Changing view acls to: kiko
25/06/01 19:40:42 INFO SecurityManager: Changing modify acls to: kiko
25/06/01 19:40:42 INFO SecurityManager: Changing view acls groups to:
25/06/01 19:40:42 INFO SecurityManager: Changing modify acls groups to:
25/06/01 19:40:42 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: kiko; groups with view permissions: EMPTY; users with modify permissions: kiko; groups with modify permissions: EMPTY
25/06/01 19:40:42 INFO Utils: Successfully started service 'sparkDriver' on port 60576.
25/06/01 19:40:42 INFO SparkEnv: Registering MapOutputTracker
25/06/01 19:40:42 INFO SparkEnv: Registering BlockManagerMaster
25/06/01 19:40:42 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
25/06/01 19:40:42 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
25/06/01 19:40:42 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
25/06/01 19:40:42 INFO DiskBlockManager: Created local directory at C:\Users\kkiko\AppData\Local\Temp\blockmgr-38ecb7ef-5f28-4434-8dd4-55fc4a1ee896
25/06/01 19:40:42 INFO MemoryStore: MemoryStore started with capacity 14.2 GiB
25/06/01 19:40:42 INFO SparkEnv: Registering OutputCommitCoordinator
25/06/01 19:40:42 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
25/06/01 19:40:42 INFO Utils: Successfully started service 'SparkUI' on port 4040.
25/06/01 19:40:42 INFO Executor: Starting executor ID driver on host Kiko.home
25/06/01 19:40:42 INFO Executor: OS info Windows 11, 10.0, amd64
25/06/01 19:40:42 INFO Executor: Java version 11.0.27
25/06/01 19:40:42 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
25/06/01 19:40:42 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@68800f8 for default.
25/06/01 19:40:42 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 60627.
25/06/01 19:40:42 INFO NettyBlockTransferService: Server created on Kiko.home:60627
25/06/01 19:40:42 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
25/06/01 19:40:42 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, Kiko.home, 60627, None)
25/06/01 19:40:42 INFO BlockManagerMasterEndpoint: Registering block manager Kiko.home:60627 with 14.2 GiB RAM, BlockManagerId(driver, Kiko.home, 60627, None)
25/06/01 19:40:42 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, Kiko.home, 60627, None)
25/06/01 19:40:42 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, Kiko.home, 60627, None)
Starting LSH Recommender with parameters:
  File Path: .\ml-20m\ml-20m\ratings.csv
  Bucket Length: 0.01
  Num Hash Tables: 30
  Distance Threshold (Cosine Similarity): 0.9
Number of ratings: 20000263

Fitting LSH model...
25/06/01 19:41:21 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS
Euclidean distance threshold for LSH join: 0.44721359549995787
Performing approximate similarity join...
25/06/01 19:41:59 WARN DAGScheduler: Broadcasting large task binary with size 31.9 MiB
25/06/01 19:42:01 WARN DAGScheduler: Broadcasting large task binary with size 31.9 MiB
25/06/01 19:42:03 WARN DAGScheduler: Broadcasting large task binary with size 31.9 MiB
25/06/01 19:42:03 WARN DAGScheduler: Broadcasting large task binary with size 31.9 MiB
25/06/01 19:43:05 WARN DAGScheduler: Broadcasting large task binary with size 32.0 MiB
25/06/01 19:43:34 WARN DAGScheduler: Broadcasting large task binary with size 32.0 MiB

--- Final RMSE of LSH-based model on test set: 0.8774219989779083 ---
Saving predictions to predictions_ratings_bl0.01_nht30_dt0.9
25/06/01 19:47:13 WARN DAGScheduler: Broadcasting large task binary with size 31.9 MiB
25/06/01 19:47:15 WARN DAGScheduler: Broadcasting large task binary with size 31.9 MiB
25/06/01 19:47:21 WARN DAGScheduler: Broadcasting large task binary with size 31.9 MiB
25/06/01 19:47:23 WARN DAGScheduler: Broadcasting large task binary with size 31.9 MiB
25/06/01 19:48:48 WARN DAGScheduler: Broadcasting large task binary with size 32.0 MiB
25/06/01 19:49:07 WARN DAGScheduler: Broadcasting large task binary with size 32.0 MiB

Stopping Spark session.